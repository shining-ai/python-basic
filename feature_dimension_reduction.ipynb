{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python機械学習クックブック"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特徴量の次元削減(特徴量抽出)\n",
    "特徴量をまとめて、次元数を落とす"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 主成分を用いた特徴量削減"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元の特徴量数: 64\n",
      "削減後の特徴量数: 54\n"
     ]
    }
   ],
   "source": [
    "# データをロード\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# 特徴量行列を標準化\n",
    "features = StandardScaler().fit_transform(digits.data)\n",
    "\n",
    "# 99%の分散を維持するPCAクラスを作成\n",
    "pca = PCA(n_components=0.99, whiten=True)\n",
    "\n",
    "# PCAを実行\n",
    "features_pca = pca.fit_transform(features)\n",
    "\n",
    "# 結果を表示\n",
    "print(\"元の特徴量数:\", features.shape[1])\n",
    "print(\"削減後の特徴量数:\", features_pca.shape[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データが線形分離不可能な際の特徴量削減"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元の特徴量数: 2\n",
      "削減後の特徴量数: 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "# 線形分離不可能なデータを作成\n",
    "features, _ = make_circles(n_samples=1000, random_state=1, noise=0.1, factor=0.1)\n",
    "\n",
    "# RBFカーネルを使用したKernelPCAで非線形次元削減を実行\n",
    "kpca = KernelPCA(kernel=\"rbf\", gamma=15, n_components=1)\n",
    "features_kpca = kpca.fit_transform(features)\n",
    "\n",
    "# 結果を表示\n",
    "print(\"元の特徴量数:\", features.shape[1])\n",
    "print(\"削減後の特徴量数:\", features_kpca.shape[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### クラスの分離性最大化による特徴量削減"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元の特徴量数: 4\n",
      "削減後の特徴量数: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.9912126])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# LDAオブジェクトを作成し、LDAを実行して特徴量を変換\n",
    "lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "features_lda = lda.fit(features, target).transform(features)\n",
    "\n",
    "# 結果を表示\n",
    "print(\"元の特徴量数:\", features.shape[1])\n",
    "print(\"削減後の特徴量数:\", features_lda.shape[1])\n",
    "\n",
    "# 変数の重要度を表す\n",
    "lda.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 必要な主成分の数を決める\n",
    "# n_componentsをNoneに設定すると、全ての主成分の寄与率が分かる\n",
    "lda = LinearDiscriminantAnalysis(n_components=None)\n",
    "features_lda = lda.fit(features, target)\n",
    "\n",
    "# 各主成分の寄与率を格納\n",
    "lda_var_ratios = lda.explained_variance_ratio_\n",
    "\n",
    "# 関数を定義\n",
    "def select_n_components(var_ratio, goal_var: float) -> int:\n",
    "    # 分散の合計\n",
    "    total_variance = 0.0\n",
    "\n",
    "    # 主成分の数\n",
    "    n_components = 0\n",
    "\n",
    "    # 分散の合計が目標の分散の合計に達するまでループ\n",
    "    for explained_variance in var_ratio:\n",
    "\n",
    "        # 分散を分散の合計に加算\n",
    "        total_variance += explained_variance\n",
    "\n",
    "        # 主成分の数をカウント\n",
    "        n_components += 1\n",
    "\n",
    "        # 目標の分散の合計に達したらループを終了\n",
    "        if total_variance >= goal_var:\n",
    "            break\n",
    "\n",
    "    # 主成分の数を返す\n",
    "    return n_components\n",
    "\n",
    "# 関数を実行\n",
    "select_n_components(lda_var_ratios, 0.95)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 行列因子分解による特徴量削減"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元の特徴量数: 64\n",
      "削減後の特徴量数: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/decomposition/_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "features = digits.data\n",
    "\n",
    "# NMFを作成し、特徴量を変換\n",
    "nmf = NMF(n_components=10, random_state=1)\n",
    "features_nmf = nmf.fit_transform(features)\n",
    "\n",
    "# 結果を表示\n",
    "print(\"元の特徴量数:\", features.shape[1])\n",
    "print(\"削減後の特徴量数:\", features_nmf.shape[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 疎データの特徴量削減"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元の特徴量数: 64\n",
      "削減後の特徴量数: 10\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# 特徴量行列の標準化\n",
    "features = StandardScaler().fit_transform(digits.data)\n",
    "\n",
    "# 疎行列を作成\n",
    "features_sparse = csr_matrix(features)\n",
    "\n",
    "# TSVDを作成し、特徴量を変換\n",
    "tsvd = TruncatedSVD(n_components=10)\n",
    "\n",
    "# 疎行列をTSVDに適合\n",
    "features_sparse_tsvd = tsvd.fit(features_sparse).transform(features_sparse)\n",
    "\n",
    "# 結果を表示\n",
    "print(\"元の特徴量数:\", features_sparse.shape[1])\n",
    "print(\"削減後の特徴量数:\", features_sparse_tsvd.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 必要な主成分の数を決める\n",
    "# 特徴量数-1を指定するとしてTSVDを実行し、元データへの寄与率を所望の値に達するために必要な成分の数を計算する\n",
    "tsvd = TruncatedSVD(n_components=features_sparse.shape[1]-1)\n",
    "features_tsvd = tsvd.fit(features)\n",
    "\n",
    "# 各主成分の寄与率を格納\n",
    "tsvd_var_ratios = tsvd.explained_variance_ratio_\n",
    "\n",
    "# 関数を定義\n",
    "def select_n_components(var_ratio, goal_var: float) -> int:\n",
    "    # 説明された寄与率の累計変数を初期化\n",
    "    total_variance = 0.0\n",
    "\n",
    "    # 主成分の数を初期化\n",
    "    n_components = 0\n",
    "\n",
    "    # 各主成分の寄与率をループ\n",
    "    for explained_variance in var_ratio:\n",
    "            \n",
    "            # 分散を分散の合計に加算\n",
    "            total_variance += explained_variance\n",
    "    \n",
    "            # 主成分の数をカウント\n",
    "            n_components += 1\n",
    "    \n",
    "            # 目標の分散の合計に達したらループを終了\n",
    "            if total_variance >= goal_var:\n",
    "                break\n",
    "\n",
    "    # 主成分の数を返す\n",
    "    return n_components\n",
    "\n",
    "# 関数を実行\n",
    "select_n_components(tsvd_var_ratios, 0.95)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特徴量の次元削減(特徴量選択)\n",
    "有効な特徴量をピックアップし、意味のない特徴量を捨てることで次元数を落とす"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数値特徴量の分散による閾値処理\n",
    "分散の小さい(あまり情報を持たない)特徴量を捨てる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 1.4, 0.2],\n",
       "       [4.9, 1.4, 0.2],\n",
       "       [4.7, 1.3, 0.2]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# 分散が0.5より大きい特徴量を削除\n",
    "thresholder = VarianceThreshold(threshold=.5)\n",
    "\n",
    "# 分散を計算\n",
    "features_high_variance = thresholder.fit_transform(features)\n",
    "\n",
    "# 分散の大きい特徴量を表示\n",
    "features_high_variance[0:3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2値特徴量の分散閾値処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "features = [[0, 1, 0],\n",
    "            [0, 1, 1],\n",
    "            [0, 1, 0],\n",
    "            [0, 1, 1],\n",
    "            [1, 0, 0]]\n",
    "\n",
    "thresholder = VarianceThreshold(threshold=(.75 * (1 - .75)))\n",
    "thresholder.fit_transform(features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 強く相関した特徴量の取り扱い"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-d013847759a4>:22: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  2\n",
       "0  1  1\n",
       "1  2  0\n",
       "2  3  1\n",
       "3  4  0\n",
       "4  5  1\n",
       "5  6  0\n",
       "6  7  1\n",
       "7  8  0\n",
       "8  9  1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = np.array(\n",
    "    [\n",
    "        [1, 1, 1],\n",
    "        [2, 2, 0],\n",
    "        [3, 3, 1],\n",
    "        [4, 4, 0],\n",
    "        [5, 5, 1],\n",
    "        [6, 6, 0],\n",
    "        [7, 7, 1],\n",
    "        [8, 7, 0],\n",
    "        [9, 7, 1],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 特徴量行列をDataFrameに変換\n",
    "dataframe = pd.DataFrame(features)\n",
    "\n",
    "# 相関行列を作成\n",
    "corr_matrix = dataframe.corr().abs()\n",
    "\n",
    "# 相関行列の上三角（右上）行列を取得\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# 相関係数が0.95より大きい特徴量のインデックスを取得\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "\n",
    "# 特徴量を削除\n",
    "dataframe.drop(dataframe.columns[to_drop], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### クラス分類に無関係な特徴量の削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元の特徴量数: 4\n",
      "削減後の特徴量数: 2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2, f_classif\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# データを整数に変換して、カテゴリ特徴量を作成\n",
    "features = features.astype(int)\n",
    "\n",
    "# カイ二乗統計量を計算\n",
    "chi2_selector = SelectKBest(chi2, k=2)\n",
    "\n",
    "# カイ二乗統計量を計算し、特徴量を2つ選択\n",
    "features_kbest = chi2_selector.fit_transform(features, target)\n",
    "\n",
    "# 結果を表示\n",
    "print(\"元の特徴量数:\", features.shape[1])\n",
    "print(\"削減後の特徴量数:\", features_kbest.shape[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元の特徴量数: 4\n",
      "削減後の特徴量数: 2\n"
     ]
    }
   ],
   "source": [
    "# F値を計算\n",
    "fvalue_selector = SelectKBest(f_classif, k=2)\n",
    "\n",
    "# 最も高いF値をもつ特徴量を2つ選択\n",
    "features_kbest = fvalue_selector.fit_transform(features, target)\n",
    "\n",
    "# 結果を表示\n",
    "print(\"元の特徴量数:\", features.shape[1])\n",
    "print(\"削減後の特徴量数:\", features_kbest.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元の特徴量数: 4\n",
      "削減後の特徴量数: 3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectPercentile\n",
    "\n",
    "# 特徴量の個数ではなく、上位n%という形で指定して特徴量を選択する事もできる\n",
    "# F値の上位75%の特徴量を選択\n",
    "fvalue_selector = SelectPercentile(f_classif, percentile=75)\n",
    "features_kbest = fvalue_selector.fit_transform(features, target)\n",
    "\n",
    "# 結果を表示\n",
    "print(\"元の特徴量数:\", features.shape[1])\n",
    "print(\"削減後の特徴量数:\", features_kbest.shape[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 再帰的な特徴量の除去"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([84, 81, 85, 75, 77,  1, 98, 36, 57, 10, 60, 69, 52,  8, 14, 65, 23,\n",
       "        6, 82, 93, 48, 50, 30, 72, 25, 68, 63,  7, 67, 80, 29, 31, 78, 95,\n",
       "       49, 88, 21, 91,  9,  1, 42, 20, 89, 19, 40, 90, 92, 12, 58, 47, 71,\n",
       "       66, 28, 70, 38, 32,  4,  5, 62, 11, 56, 94, 59, 26, 74, 97, 18, 37,\n",
       "       41, 55, 79, 53, 13, 86, 34,  2, 61, 46, 45, 51, 83, 54,  3, 27, 15,\n",
       "       22, 64, 99, 96, 43, 24, 33, 17, 39, 87, 73, 35, 44, 16, 76])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn import datasets, linear_model\n",
    "\n",
    "# 警告を無視\n",
    "warnings.filterwarnings(\n",
    "    action=\"ignore\", module=\"scipy\", message=\"^internal gelsd\"\n",
    ")\n",
    "\n",
    "# 特徴量行列とターゲットベクトルを作成\n",
    "features, target = make_regression(\n",
    "    n_samples=10000, n_features=100, n_informative=2, random_state=1\n",
    ")\n",
    "\n",
    "# 線形回帰器を作成\n",
    "ols = linear_model.LinearRegression()\n",
    "\n",
    "# 再帰的に特徴量を削減\n",
    "rfecv = RFECV(estimator=ols, step=1, scoring=\"neg_mean_squared_error\")\n",
    "rfecv.fit(features, target)\n",
    "rfecv.transform(features)\n",
    "\n",
    "# 最適な特徴量の個数\n",
    "rfecv.n_features_\n",
    "\n",
    "# 最適な特徴量を表示\n",
    "rfecv.support_\n",
    "\n",
    "# 最適な特徴量のランキングを表示\n",
    "rfecv.ranking_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
